---
layout: page
title: Ph.D. thesis
---

<h3>Introduction</h3>
<p>The Fourth Industrial Revolution (Industry 4.0) is a technology-driven revolution that seeks to transform processes,
    systems, and methods through the digitalization and interconnectivity of all assets. Smart Factories, an essential
    pillar of Industry 4.0, require the integration of multiple technologies that lead to an interoperable
    manufacturing plant where almost all components of the value network are connected, coordinated, and share data.
    Artificial Intelligence (AI) has become one of the main vectors for this revoulolution, but one obstacle to its use
    in industry is that algorithms are highly "data consuming."
</p>
<p>
    Unlike other machine learning approaches that rely in a model gorund-truth dataset, Reinforcement Learning (RL) and
    Deep Reinforcement Learning (DRL) algorithms learn thanks to the experience gathered from the interaction of an
    agent with its environment. Although most of the industry processes depend on classic control algorithms that
    exploits the knowledge of the operation dynamics, we argue that RL and DRL approaches can address the same problems,
    and more complex ones that are currently unsolved, introducing generalization capabilities, scalability and
    flexibility into the current industrial environment constraints. However, the major constraint of the application of
    RL to industry problems is the amount of experience an agent consumes until it learns.
</p>
<p>
    This <b>sample efficiency</b> issue of RL algorithms can be addressed relying on the use of virtual environments to
    train the agents. The ideal case would rely on a simulator that is an exact reproduction of the physical setup, but
    in most cases, there will be divergences between both layouts. These divergences can make agents not perform as
    expected when they are deployed in the real environment. To deal with this problem, several <b>transfer learning
        techniques</b> have been developed to bridge the gap between the real world and simulation, known as
    <b>sim-to-real</b>.
</p>
<p>
    Therefore, with this approach the sample efficiency problem is mitigated, but another one raises: <u>the need to
        efficiently transfer the synthetic experience into the real world</u>. Within this framework, my Ph.D. thesis is
    framed as <b>the search for an optimized pipeline to solve the sim-to-real problem</b>.
</p>
<p>
<div class="image"><img src="images/pic_Thesis_1.png" alt="" style="width: 80%; margin: auto;" /></div>
</p>

<hr />
<h3>Ph.D. Thesis objectives and contributions</h3>
<p>According to state-of-the-art transfer learning techniques, we argue that there is not a single technique that might
    tackle properly the problem, so the optimal sim-to-real solution should be an approach that integrates various
    methods to boost their strengths and compensate their weaknesses.
</p>
<p>
    The ultimate goal is to achieve a procedure that endows resulting models with optimized generalization properties,
    seeking to avoid the dependency on photorealistic simulators which might increase the computational resources
    needed, and the use of complex algorithms that might result in task or environment overfitting.
</p>
<p>
    Hence the Ph.D. Thesis objectives and contributions might be sum-up as follows:
<h5>Objectives</h5>
<ol>
    <li>Design an optimized pipeline for the transferring experience process between (deep) reinforcement learning
        agents.</li>
    <li>Validate the optimized pipeline and its generalization capabilities.</li>
    <li>Define the methodology that needs to be followed to implement the optimized pipeline in a completely different
        industrial application.</li>
</ol>
<h5>Contributions</h5>
<ul>
    <li>Based on a detailed analysis of the transfer learning techniques, select the most suitable and
        interesting procedures that might be implemented in an industrial application.</li>
    <li>Use Domain Randomization to speed up the transfer of knowledge process and quantify the benefit of adding
        diversity in a sim-to-sim approach.</li>
    <li>An optimized pipeline that efficiently bridges the reality gap and is appropriate for industrial use cases.</li>
    <li>Validation of the proposed tool with two different robots and a descriptive methodology that should be followed
        to implement it in another industrial process.</li>
    <li>Analysis of the drivers that interfere with and hinder transferring experience among simulated and real assets.
    </li>
</ul>
</p>

<hr />
<h3>Ph.D. Thesis case study and assets</h3>
<h5>Case study</h5>
<p> The selected industrial operation is Pick & Place, the task where robotic arms pick a component and place it
    anywhere else. It was selected because: 1) currently, it is a problem that is not solved for chaotic environments;
    2) it is a cross-cutting process in many industries; and 3) we can access to the assets that comprise the challenge,
    i.e. two different robotic arms. </p>
<p>
    We are first focusing on the algorithm that learns how to approach the arm to a target that is positioned randomly
    within the robot workspace area. The robotic arm starts with a random initial pose. This framework corresponds to
    the first part of a Pick & Place process.
    The input to the agent is just the environment image from which it must infer the robot and the target pose. Since
    the input is a picture, the representation learning process is embedded in the model due to the state's high
    dimensionality.
</p>
<h5>Assets</h5>
<p>We have chosen two different robotic arms, an industrial robot, the <a target="_blank"
        href="https://new.abb.com/products/robotics/robots/articulated-robots/irb-120">IRB120
        by ABB</a> and a collaborative one, the <a target="_blank"
        href="https://www.universal-robots.com/products/ur3-robot/">UR3e by Universal Robots</a>. Both of these models
    have 6 degrees of freedom, but they differ significantly in terms of their morphology. We believe that these
    differences will enable us to test our proposal under two distinct environments, while still allowing the agent to
    perform the same task. This will enable us to evaluate the performance of our solution in real-world scenarios and
    develop a solution that is scalable and applicable in various industry problems.
</p>
<p>
<div class="image fit"><img src="images/pic_RobotsEnv.png" alt="" style="width: 80%; margin: auto;" /></div>
</p>

<hr />
<h3>From concepts to reality: Milestones achieved so far</h3>
<p>In our search for an efficient solution for the sim-to-real problem that integrates transfer learning techniques, we
    have been exploring the intersection between <b>Progressive Neural Networks (PNNs)</b> and <b>Domain Randomization
        (DR)</b>. Regarding this analysis, we have already published a paper titled: <a target="_blank"
        href="https://link.springer.com/article/10.1007/s10489-022-04227-3"><i>"Learning more with the same
            effort: how randomization improves the robustness of a robotic deep reinforcement learning agent"</i> in
        Applied Intelligence (Springer)</a>.
</p>
<p>
    Regarding the blending of transfer learning techniqes, we also plan to add <b>Domain Adaptation (DA)</b> to the
    equation and some <b>object-centric ideas</b> in the near future.
</p>
<p>
    Currently, our focus is on investigating the learning mechanisms of the agents that have been trained using the PNN
    architecture. This is being done to gain deeper insights into how, when, and where knowledge transfer takes place.
    Our paper titled <a target="_blank"
        href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4342081"><i>"Evaluating the perception, understanding,
            and forgetting of Progressive Neural Networks: a quantitative and qualitative analysis,"</i> currently under
        review by the Neural Networks journal,</a> presents our findings in this line.
</p>
<p>
    We have also conducted <b>zero-shot experiments</b> on the IRB120 robot and are currently in the process of
    deploying the PNN architecture in real-world settings to evaluate its sim-to-real efficiency.
    On the other hand, we are also researching and testing <b>Bayesian Optimization techniques</b> to define a
    methodology that results in an optimum set of hyperparameters for a DRL model.
</p>

<hr />
<h3>Publications</h3>
<p>
<ul>
    <li><a target="_blank" href="https://link.springer.com/article/10.1007/s10489-022-04227-3">Güitta-López, L., Boal,
            J. & López-López, Á.J. Learning more with the same effort: how randomization improves
            the robustness of a robotic deep reinforcement learning agent. Applied Intelligence (2022).</a>
        <a target="_blank"
            href="https://doi.org/10.1007/s10489-022-04227-3">https://doi.org/10.1007/s10489-022-04227-3</a>
    </li>
    <li><a target="_blank" href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4342081">Güitta-López, Lucía and
            Boal Martín-Larrauri, Jaime and López López, Álvaro Jesús, Evaluating the Perception,
            Understanding, and Forgetting of Progressive Neural Networks: A Quantitative and Qualitative Analysis.</a>
        Available at SSRN: <a target="_blank" href="https://ssrn.com/abstract=4342081">ssrn.com/abstract=4342081</a>
        or <a target="_blank" href="http://dx.doi.org/10.2139/ssrn.4342081">dx.doi.org/10.2139/ssrn.4342081</a>
    </li>
</ul>
</p>

<hr />
<p>
    <a href="/index.html" class="button bottom">Home</a>
</p>