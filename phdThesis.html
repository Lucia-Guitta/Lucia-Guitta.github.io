---
layout: page
title: Ph.D. thesis
---

<h3>Introduction</h3>
<p>The Fourth Industrial Revolution (Industry 4.0) is a technology-driven revolution that seeks to transform processes,
    systems, and methods through the digitalization and interconnectivity of all assets. Smart Factories, an essential
    pillar of Industry 4.0, require the integration of multiple technologies that lead to an interoperable
    manufacturing plant where almost all components of the value network are connected, coordinated, and share data.
    Artificial Intelligence (AI) has become one of the main vectors for this revoulolution, but one obstacle to its use
    in industry is that algorithms are highly "data consuming."
</p>
<p>
    Unlike other machine learning approaches that rely in a model gorund-truth dataset, Reinforcement Learning (RL) and
    Deep Reinforcement Learning (DRL) algorithms learn thanks to the experience gathered from the interaction of an
    agent with its environment. Although most of the industry processes depend on classic control algorithms that
    exploits the knowledge of the operation dynamics, we argue that RL and DRL approaches can address the same problems,
    and more complex ones that are currently unsolved, introducing generalization capabilities, scalability and
    flexibility into the current industrial environment constraints. However, the major constraint of the application of
    RL to industry problems is the amount of experience an agent consumes until it learns.
</p>
<p>
    This <b>sample efficiency</b> issue of RL algorithms can be addressed relying on the use of virtual environments to
    train the agents. The ideal case would rely on a simulator that is an exact reproduction of the physical setup, but
    in most cases, there will be divergences between both layouts. These divergences can make agents not perform as
    expected when they are deployed in the real environment. To deal with this problem, several <b>transfer learning
        techniques</b> have been developed to bridge the gap between the real world and simulation, known as
    <b>sim-to-real</b>.
</p>
<p>
    Therefore, with this approach the sample efficiency problem is mitigated, but another one raises: <u>the need to
        efficiently transfer the synthetic experience into the real world</u>. Within this framework, my Ph.D. thesis is
    framed as <b>the search for an optimized pipeline to solve the sim-to-real problem</b>.
</p>
<p>
<div class="image"><img src="images/pic_Thesis_1.jpeg" alt="" height="700" /></div>
</p>
<hr />
<h3>Ph.D. Thesis objectives and contributions</h3>
<p>According to state-of-the-art transfer learning techniques, we argue that there is not a single technique that might
    tackle properly the problem, so the optimal sim-to-real solution should be an approach that integrates various
    methods to boost their strengths and compensate their weaknesses.
</p>
<p>
    Hence the Ph.D. Thesis objectives and contributions might be sum-up as follows:
<h5>Objectives</h5>
<ol>
    <li>Design an optimized pipeline for the transferring experience process between (deep) reinforcement learning
        agents.</li>
    <li>Validate the optimized pipeline and its generalization capabilities.</li>
    <li>Define the methodology that needs to be followed to implement the optimized pipeline in a completely different
        industrial application.</li>
</ol>
<h5>Contributions</h5>
<ul>
    <li>Based on a detailed analysis of the hybrid transfer learning techniques, select the most suitable and
        interesting procedures that might be implemented in an industrial application.</li>
    <li>Use of Domain Randomization to speed up the transfer of knowledge process and quantify the benefit of adding
        diversity in a sim-to-sim approach.</li>
    <li>An optimized pipeline that efficiently bridges the reality gap and is appropriate for industrial use cases.</li>
    <li>Validation of the proposed tool with two different robots and a descriptive methodology that should be followed
        to implement it in another industrial process (in this case, a general process that does not need to be a
        robot).</li>
    <li>Analysis of the drivers that interfere with and hinder transferring experience among simulated and real assets.
    </li>
</ul>
</p>
<hr />
<h3>Ph.D. Thesis assets</h3>
<p>According to state-of-the-art transfer learning techniques, we argue that there is not a single technique that might
    tackle properly the problem, so the optimal sim-to-real solution should be an approach that integrates various
    methods to boost their strengths and compensate their weaknesses. Particularly, we have been exploring the
    intersection between Progressive Neural Networks (PNNs) and Domain Randomization (DR), although we plan to add
    Domain Adaptation (DA) to the equation and some object-centric concepts. The ultimate goal is to achieve a procedure
    that endows resulting models with optimized generalization properties, seeking to avoid the dependency on
    photorealistic simulators which might increase the computational resources needed, and the use of complex algorithms
    that might result in task or environment overfitting.
</p>
<hr />
<h3>From idea to reality: Milestones achieved so far</h3>
<p>According to state-of-the-art transfer learning techniques, we argue that there is not a single technique that might
    tackle properly the problem, so the optimal sim-to-real solution should be an approach that integrates various
    methods to boost their strengths and compensate their weaknesses. Particularly, we have been exploring the
    intersection between Progressive Neural Networks (PNNs) and Domain Randomization (DR), although we plan to add
    Domain Adaptation (DA) to the equation and some object-centric concepts. The ultimate goal is to achieve a procedure
    that endows resulting models with optimized generalization properties, seeking to avoid the dependency on
    photorealistic simulators which might increase the computational resources needed, and the use of complex algorithms
    that might result in task or environment overfitting.
</p>